_target_: g2p.model.G2p
max_len: 48
encoder:
  _target_: g2p.model.Encoder
  graphemes:
    - "<unk>"
    - "<pad>"
    - "<bos>"
    - "<eos>"
    - "'"
    - "-"
    - "A"
    - "B"
    - "C"
    - "D"
    - "E"
    - "F"
    - "G"
    - "H"
    - "I"
    - "J"
    - "K"
    - "L"
    - "M"
    - "N"
    - "O"
    - "P"
    - "Q"
    - "R"
    - "S"
    - "T"
    - "U"
    - "V"
    - "W"
    - "X"
    - "Y"
    - "Z"
  d_model: 64
  d_hidden: 128
  num_layers: 2
  dropout: 0.1
decoder:
  _target_: g2p.model.Decoder
  phonemes:
    - "<unk>"
    - "<pad>"
    - "<bos>"
    - "<eos>"
    - "AA"
    - "AE"
    - "AH"
    - "AO"
    - "AW"
    - "AY"
    - "B"
    - "CH"
    - "D"
    - "DH"
    - "EH"
    - "ER"
    - "EY"
    - "F"
    - "G"
    - "HH"
    - "IH"
    - "IY"
    - "JH"
    - "K"
    - "L"
    - "M"
    - "N"
    - "NG"
    - "OW"
    - "OY"
    - "P"
    - "R"
    - "S"
    - "SH"
    - "T"
    - "TH"
    - "UH"
    - "UW"
    - "V"
    - "W"
    - "Y"
    - "Z"
    - "ZH"
  d_model: 64
  d_hidden: 128
  num_layers: 2
  dropout: 0.1

optimizer:
  _target_: pytorch_optimizer.DAdaptAdam
  lr: 1.0
  betas: [0.9, 0.999]
  

lr_scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  mode: 'min'
  factor: 0.4
  patience: 20
  min_lr: 5e-6
  verbose: True
